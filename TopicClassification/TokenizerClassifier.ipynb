{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3683190f-3c83-4366-90c7-9805f1db99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como el limite establecido anteriormente es de 2500 tweets por archivo pickle, se han hecho 10 extracciones a tiempos diferentes para disponer de 25000 tweets (por tópico)\n",
    "\n",
    "fileName = ['electronics_train_data.data', 'politics_train_data.data', 'travel_train_data.data', 'car_train_data.data',\n",
    "            'electronics_and_car_train_data.data', 'politics_and_car_train_data.data', 'travel_and_car_train_data.data',\n",
    "            'politics_and_travel_train_data.data', 'travel_and_electronics_train_data.data']\n",
    "\n",
    "fileName1 = ['electronics_train_data1.data', 'politics_train_data1.data', 'travel_train_data1.data', 'car_train_data1.data',\n",
    "            'electronics_and_car_train_data1.data', 'politics_and_car_train_data1.data', 'travel_and_car_train_data1.data',\n",
    "            'politics_and_travel_train_data1.data', 'travel_and_electronics_train_data1.data']\n",
    "\n",
    "fileName2 = ['electronics_train_data2.data', 'politics_train_data2.data', 'travel_train_data2.data', 'car_train_data2.data',\n",
    "            'electronics_and_car_train_data2.data', 'politics_and_car_train_data2.data', 'travel_and_car_train_data2.data',\n",
    "            'politics_and_travel_train_data2.data', 'travel_and_electronics_train_data2.data']\n",
    "\n",
    "fileName3 = ['electronics_train_data3.data', 'politics_train_data3.data', 'travel_train_data3.data', 'car_train_data3.data',\n",
    "            'electronics_and_car_train_data3.data', 'politics_and_car_train_data3.data', 'travel_and_car_train_data3.data',\n",
    "            'politics_and_travel_train_data3.data', 'travel_and_electronics_train_data3.data']\n",
    "\n",
    "fileName4 = ['electronics_train_data4.data', 'politics_train_data4.data', 'travel_train_data4.data', 'car_train_data4.data',\n",
    "            'electronics_and_car_train_data4.data', 'politics_and_car_train_data4.data', 'travel_and_car_train_data4.data',\n",
    "            'politics_and_travel_train_data4.data', 'travel_and_electronics_train_data4.data']\n",
    "\n",
    "fileName5 = ['electronics_train_data5.data', 'politics_train_data5.data', 'travel_train_data5.data', 'car_train_data5.data',\n",
    "            'electronics_and_car_train_data5.data', 'politics_and_car_train_data5.data', 'travel_and_car_train_data5.data',\n",
    "            'politics_and_travel_train_data5.data', 'travel_and_electronics_train_data5.data']\n",
    "\n",
    "fileName6 = ['electronics_train_data6.data', 'politics_train_data6.data', 'travel_train_data6.data', 'car_train_data6.data',\n",
    "            'electronics_and_car_train_data6.data', 'politics_and_car_train_data6.data', 'travel_and_car_train_data6.data',\n",
    "            'politics_and_travel_train_data6.data', 'travel_and_electronics_train_data6.data']\n",
    "\n",
    "fileName7 = ['electronics_train_data7.data', 'politics_train_data7.data', 'travel_train_data7.data', 'car_train_data7.data',\n",
    "            'electronics_and_car_train_data7.data', 'politics_and_car_train_data7.data', 'travel_and_car_train_data7.data',\n",
    "            'politics_and_travel_train_data7.data', 'travel_and_electronics_train_data7.data']\n",
    "\n",
    "fileName8 = ['electronics_train_data8.data', 'politics_train_data8.data', 'travel_train_data8.data', 'car_train_data8.data',\n",
    "            'electronics_and_car_train_data8.data', 'politics_and_car_train_data8.data', 'travel_and_car_train_data8.data',\n",
    "            'politics_and_travel_train_data8.data', 'travel_and_electronics_train_data8.data']\n",
    "\n",
    "fileName9 = ['electronics_train_data9.data', 'politics_train_data9.data', 'travel_train_data9.data', 'car_train_data9.data',\n",
    "            'electronics_and_car_train_data9.data', 'politics_and_car_train_data9.data', 'travel_and_car_train_data9.data',\n",
    "            'politics_and_travel_train_data9.data', 'travel_and_electronics_train_data9.data']\n",
    "\n",
    "labelsFile = [['electronics'], ['politics'], ['travel'], ['car'],\n",
    "              ['electronics','car'], ['politics','car'], ['travel','car'],\n",
    "              ['politics','travel'], ['travel','electronics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c200af0-9ef5-418f-93d5-1c8edc435c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "#Se definen las funciones usadas para leer la información de los archivos pickle\n",
    "\n",
    "def getCombineTweetsPerUser(tweetList):\n",
    "  result = ''\n",
    "  for tweet in tweetList:\n",
    "    result = result + ' '\n",
    "    result = result + tweet\n",
    "  return result\n",
    "  \n",
    "def readPickleFiles(fileName):\n",
    "  labelData = []\n",
    "  contentData = []\n",
    "  for index in range(len(fileName)):\n",
    "    pkl_file = open('./' + fileName[index],'rb')\n",
    "    getDict = pickle.load(pkl_file)\n",
    "    for user in getDict:\n",
    "      aggregatedDoc = getCombineTweetsPerUser(getDict[user])\n",
    "      contentData.append(aggregatedDoc)\n",
    "      labelData.append(labelsFile[index])\n",
    "  return labelData,contentData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65a1bda2-d053-4287-956c-e3e14598baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se asigna un nombre a cada grupo de etiquetas y de contenido (el texto del tweet) y más tarde agregan todas en una única lista (una para todas las etiquetas, \n",
    "#otra para todos los contenidos)\n",
    "\n",
    "label, Content = readPickleFiles(fileName)\n",
    "label1, Content1 = readPickleFiles(fileName1)\n",
    "label2, Content2 = readPickleFiles(fileName2)\n",
    "label3, Content3 = readPickleFiles(fileName3)\n",
    "label4, Content4 = readPickleFiles(fileName4)\n",
    "label5, Content5 = readPickleFiles(fileName5)\n",
    "label6, Content6 = readPickleFiles(fileName6)\n",
    "label7, Content7 = readPickleFiles(fileName7)\n",
    "label8, Content8 = readPickleFiles(fileName8)\n",
    "label9, Content9 = readPickleFiles(fileName9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d24cc254-700a-447c-8571-f6cfd9607ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.extend(label1)\n",
    "label.extend(label2)\n",
    "label.extend(label3)\n",
    "label.extend(label4)\n",
    "label.extend(label5)\n",
    "label.extend(label6)\n",
    "label.extend(label7)\n",
    "label.extend(label8)\n",
    "label.extend(label9)\n",
    "\n",
    "Content.extend(Content1)\n",
    "Content.extend(Content2)\n",
    "Content.extend(Content3)\n",
    "Content.extend(Content4)\n",
    "Content.extend(Content5)\n",
    "Content.extend(Content6)\n",
    "Content.extend(Content7)\n",
    "Content.extend(Content8)\n",
    "Content.extend(Content9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f01caf00-57cb-4712-9dfa-ba27edaeb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se utiliza train_test_split para dividir la información en train y test (0.8 y 0.2 respectivamente)\n",
    "\n",
    "trainContent, testContent, labelTrain, labelTest = model_selection.train_test_split(Content, label, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f438217-e0da-4cff-bcd2-2dc907677817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Se usa nltk para usar su lista de stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e69f2f9e-2758-4865-a2ef-71e04b2ba19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Se definen algunas funciones para tratar el testo de los tweets: la primera función elimina todos los caracteres que no sean alfabeticos mientras que la segunda elimina,\n",
    "#todas las stopwords de la lista de nltk\n",
    "\n",
    "def getOnlyWords(wordsList):\n",
    "  pattern = re.compile('[a-zA-Z]+')\n",
    "  result = []\n",
    "  for word in wordsList:\n",
    "    if pattern.match(word) != None:\n",
    "      result.append(word)\n",
    "  return result\n",
    "  \n",
    "def getWordsWithoutStop(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  words = word_tokenize(sentence)\n",
    "  withoutStopWords = [word for word in words if not word in stopWords]\n",
    "  return getOnlyWords(withoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "672a027e-2d49-4aab-aaf4-dc3647eff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez hecho el preprocesamiento de la inforamción se vectoriza de manera separada el train y test\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = getWordsWithoutStop)\n",
    "vectorizedTrainData = vectorizer.fit_transform(trainContent)\n",
    "vectorizedTestData = vectorizer.transform(testContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e9ce393b-4543-4022-9c70-81c3e3b56a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#Esto transforma las listas de etiquetas train y test que se tenían anteriormente en un problema de tipo multilabel (múltiples etiquetas)\n",
    "\n",
    "multiLabelBinary = MultiLabelBinarizer()\n",
    "trainBinaryLabel = multiLabelBinary.fit_transform(labelTrain)\n",
    "testBinaryLabel = multiLabelBinary.transform(labelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "475aee5f-e7fc-4767-a540-5cf92a9164a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain multi-label Test micro,\n",
      "0.34160566912600976\n",
      "ClassifierChain multi-label Test macro,\n",
      "0.3256029223292891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Primero se hace un fit de una cadena de clasificadores de tipo SVC linear y se extrae la predicción media de la cadena\n",
    "\n",
    "chains = [ClassifierChain(LinearSVC(), order='random', random_state=i)\n",
    "          for i in range(20)]\n",
    "for chain in chains:\n",
    "  chain.fit(vectorizedTrainData,trainBinaryLabel)\n",
    "\n",
    "# Se hacen predicciones de los datos del train\n",
    "\n",
    "pred_train_chains = np.array([chain.predict(vectorizedTrainData) \n",
    "                             for chain in chains])\n",
    "\n",
    "\n",
    "#Aqui se busca encontrar el valor óptimo del umbral que maximice la precisión del training set, por lo que se hace una media de las predicciones de las cadenas con cada \n",
    "#valor elegido de umbral (threshold)\n",
    "\n",
    "pred_train_ensemble = pred_train_chains.mean(axis=0)\n",
    "threshold = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "optimal_threshold = 0.0\n",
    "max_precision = 0.0\n",
    "\n",
    "for thres in threshold:\n",
    "  curPredictionsTrain = (pred_train_ensemble >= thres)\n",
    "  precisionTrain = precision_score(trainBinaryLabel, curPredictionsTrain, average = 'micro')\n",
    "  precisionTrain = precision_score(trainBinaryLabel, curPredictionsTrain, average = 'macro')\n",
    "  if max_precision < precisionTrain:\n",
    "    max_precision = precisionTrain\n",
    "    optimal_threshold = thres\n",
    "\n",
    "# Una vez se ha encontrado el valor óptimo del umbral, se usa para hacer las predicciones del test set\n",
    "pred_test_chains = np.array([chain.predict(vectorizedTestData) \n",
    "                             for chain in chains])\n",
    "pred_test_ensemble = pred_test_chains.mean(axis=0)\n",
    "pred_test_ensemble = (pred_test_ensemble >= optimal_threshold)\n",
    "\n",
    "# Aqui se obtienen los resultados del test set que vienen del ensemble learning\n",
    "precisionTest = precision_score(testBinaryLabel, pred_test_ensemble, average = 'micro')\n",
    "print (\"ClassifierChain multi-label Test micro,\")\n",
    "print (precisionTest)\n",
    "\n",
    "precisionTest = precision_score(testBinaryLabel, pred_test_ensemble, average = 'macro')\n",
    "print (\"ClassifierChain multi-label Test macro,\")\n",
    "print (precisionTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb204f3-e4e1-457d-b32d-b1a24756d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta es la funcion que te dice según los resultados obtenidos del modelo, qué tópico es mas probable que describa al tweet (se introduce en la función en forma de string)\n",
    "def getTopicFromTweet(tweet):    \n",
    "    vectorizedTweetData = vectorizer.transform(list(tweet))\n",
    "    pred_test_chains = np.array([chain.predict(vectorizedTweetData) \n",
    "                                 for chain in chains])\n",
    "    pred_test_ensemble = pred_test_chains.mean(axis=0)\n",
    "    array=pred_test_ensemble.mean(axis=0)\n",
    "    resultados=(('Car',array[0]),('Electronics',array[1]),('Politics',array[2]),('Travel',array[3]))\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674e9a6-a463-4ca5-91ec-dc197ca9abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados=getTopicFromTweet(tweet='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9e1eb-2637-4274-ae89-30dacec203cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como comentario sobre los resultados se puede ver que el valor obtenido no es demasiado satisfactorio. Se ha probado con diferentes cantidades de train data y se ha \n",
    "#comprobado que el valor de precisión del test ha subido con más datos (alrededor de 0.25 con 2500 tweets por tópico y alrededor de 0.35 con 25000 tweets por tópico) pero\n",
    "#dicho valor sigue sin acercarse a lo que se podría considerar un modelo fiable. \n",
    "\n",
    "#Como futuros trabajos, se podría intentar mejorar los resultados usando NN o redes preentrenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f38edc-0d53-43d1-866b-1450ca88ac98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19300d-6173-4f50-973a-22c87b4b2b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7359a-70b7-45f3-a559-6df6b2a3f9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd190cc7-c513-49ae-88af-f9bc05d5d6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
